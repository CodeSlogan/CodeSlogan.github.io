<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>论文阅读Neural Association Models | CodeSlogan</title><meta name="keywords" content="ML"><meta name="author" content="codeslogan"><meta name="copyright" content="codeslogan"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文题目归属及框架    背景与动机 在本文中，作者提出了一种新的深度学习方法——神经关联模型(NAM)，用于AI中的概率推理。作者建议使用神经网络来模拟一个领域中任意两个事件之间的关联。神经网络将一个事件作为输入，并计算出另一个事件的条件概率，以模拟这两个事件关联的可能性。与现有的线性模型不同，该NAM模型利用的是深度神经网络中的多层非线性激活来模拟它们的关联。 在这项工作中，作者研究了两种NA">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读Neural Association Models">
<meta property="og:url" content="http://example.com/2021/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBNeural%20Association%20Models/index.html">
<meta property="og:site_name" content="CodeSlogan">
<meta property="og:description" content="论文题目归属及框架    背景与动机 在本文中，作者提出了一种新的深度学习方法——神经关联模型(NAM)，用于AI中的概率推理。作者建议使用神经网络来模拟一个领域中任意两个事件之间的关联。神经网络将一个事件作为输入，并计算出另一个事件的条件概率，以模拟这两个事件关联的可能性。与现有的线性模型不同，该NAM模型利用的是深度神经网络中的多层非线性激活来模拟它们的关联。 在这项工作中，作者研究了两种NA">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img-blog.csdnimg.cn/67f7a84d5cb04facb21f8cd45dc965ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202153543459">
<meta property="article:published_time" content="2021-12-02T16:10:20.000Z">
<meta property="article:modified_time" content="2021-12-03T01:03:55.012Z">
<meta property="article:author" content="codeslogan">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/67f7a84d5cb04facb21f8cd45dc965ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202153543459"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2021/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBNeural%20Association%20Models/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读Neural Association Models',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-03 09:03:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.2.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/1.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://img-blog.csdnimg.cn/67f7a84d5cb04facb21f8cd45dc965ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center&quot; alt=&quot;image-20211202153543459')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">CodeSlogan</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读Neural Association Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-02T16:10:20.000Z" title="发表于 2021-12-03 00:10:20">2021-12-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-03T01:03:55.012Z" title="更新于 2021-12-03 09:03:55">2021-12-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/KG/">KG</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>5分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读Neural Association Models"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1>论文题目归属及框架</h1>
<img src="https://img-blog.csdnimg.cn/74735b4af1254f35887372b6358f2a42.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211201192508450" width="67%;">
<img src="https://img-blog.csdnimg.cn/83e142dc1b5b4130830ba1e124731a56.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211201192540544" width="50%;">
<img src="https://img-blog.csdnimg.cn/a6597ea7b642430d8b80faaba54efca3.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16" alt="Probabilistic Reasoning via Deep Learning" style="80%;">
<h1>背景与动机</h1>
<p>在本文中，作者提出了一种新的深度学习方法——<strong>神经关联模型(NAM)</strong>，用于AI中的<strong>概率推理</strong>。作者建议使用神经网络来模拟一个领域中任意两个事件之间的关联。神经网络将一个事件作为输入，并计算出另一个事件的<strong>条件概率</strong>，以模拟这两个事件关联的可能性。与现有的线性模型不同，该NAM模型利用的是深度神经网络中的<strong>多层非线性激活</strong>来模拟它们的关联。</p>
<p>在这项工作中，作者研究了两种NAM模型结构。第一个模型是标准的深度神经网络(<strong>DNN</strong>)，第二个模型使用一种特殊的结构称为关系调制神经网络(<strong>RMNN</strong>)。</p>
<p>在多种知识图谱任务上进行实验后表明，两个模型均能优于其它常规的方法。更值得一提的是，RMNN在知识迁移学习方面的表现尤为突出，在只观察少量训练样本的情况下，预先训练的模型可以迅速扩展到新的关系。</p>
<h1>贡献</h1>
<ul>
<li>作者提出了基于深度神经网络的概率推理神经关联模型，该模型具有足够的通用性，可以处理符号事件的各种推理问题。</li>
<li>本工作研究的一种特定的模型(RMNN)对于知识转移学习非常有效，它可以快速地将现有的知识库适应于新遇到的场景和情境。</li>
<li>第一个证明DNNs具有多层非线性的工作</li>
</ul>
<h1>统计关系学习</h1>
<p>统计关系学习，Statistical Relation Learning(SRL)，</p>
<p>SRL试图将实体集E和关系集R上的每一个可能的三元组xijk = (ei, rk, ej)建模为二元随机变量yijk∈{0,1}，表示真或假。</p>
<p>SRL模型的似然函数如下:</p>
<img src="https://img-blog.csdnimg.cn/519ea9e159bf45838d522eefe29b2984.png#pic_center" alt="image-20211202152510779" width="67%;">
<p>σ(·)表示sigmoid function</p>
<p>Ber (y|p)为伯努利分布</p>
<img src="https://img-blog.csdnimg.cn/ea9c84d2f2474fa39520c2d2f37356a1.png#pic_center" alt="image-20211202152618773" width="50%;">
<p>该函数的输入为一个三元组向量，通过score function<code>f()</code>，来计算其得分。把得分投到sigmoid激活函数中，实现归一化，用以表示概率</p>
<h1>NAM</h1>
<img src="https://img-blog.csdnimg.cn/67f7a84d5cb04facb21f8cd45dc965ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202153543459" width="50%;">
<p>NAM模型以事件E1的向量作为输入，计算E2事件的条件概率</p>
<p>如果事件 E2 是二值的（True or False），NAM 用 sigmoid 函数计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><mi>E</mi><mn>2</mn><mi mathvariant="normal">∣</mi><mi>E</mi><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr(E2|E1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">2∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">1</span><span class="mclose">)</span></span></span></span>；如果 E2 是多值的，则 NAM 使用 softmax 计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>r</mi><mo stretchy="false">(</mo><mi>E</mi><mn>2</mn><mi mathvariant="normal">∣</mi><mi>E</mi><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Pr(E2|E1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">2∣</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mord">1</span><span class="mclose">)</span></span></span></span>，输出多值向量。</p>
<p>事件E1，E2的具体含义会根据所训练的任务的不同而发生改变，常见的情况有如下5种：</p>
<img src="https://img-blog.csdnimg.cn/8c35ada173a14ef1a90ae9f015431918.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202153908233" width="50%;">
<h2 id="loss">loss</h2>
<p>NAM 的 loss 采用了 log likelihood 函数，即对数似然损失函数</p>
<img src="https://img-blog.csdnimg.cn/e3a5951e1b714a5e9e9011122193c4f7.png#pic_center" alt="image-20211202154313863" width="50%;">
<p>从形式上不难判断出，该函数在参数为<code>Θ</code>的情况下，分别计算了数据集中每个正样本和负样本的logistic score之和</p>
<p>训练方法：随机梯度下降</p>
<h1>DNN</h1>
<img src="https://img-blog.csdnimg.cn/b3e981c421e141eaa5c433a23b885923.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width="50%;">
<img src="https://img-blog.csdnimg.cn/ff7daaf95a9f468b9f1b632bb96e04f9.png#pic_center" alt="image-20211202161527504" width="30%;">
<p>模型的每一层的输出由头实体向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><mi>i</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">v_i^{(1)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3217em;vertical-align:-0.2769em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2769em;"><span></span></span></span></span></span></span></span></span></span>和关系向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">c_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>组成</p>
<img src="https://img-blog.csdnimg.cn/b28341eab94243099f6ab4238c865c5f.png#pic_center" width="50%;">
<p>W和b分别表示了每一层的权重矩阵与偏重，计算得出a</p>
<img src="https://img-blog.csdnimg.cn/7183ed50ff444060a6fe7da8bf1b2979.png#pic_center" alt="image-20211202161558040" width="50%;">
<p>最后，作者使用最后一层的输出z和尾实体向量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>v</mi><mi>j</mi><mrow><mo stretchy="false">(</mo><mn>2</mn><mo stretchy="false">)</mo></mrow></msubsup></mrow><annotation encoding="application/x-tex">v_j^{(2)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.4578em;vertical-align:-0.413em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0448em;"><span style="top:-2.4231em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span><span style="top:-3.2198em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mtight">2</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.413em;"><span></span></span></span></span></span></span></span></span></span>，代入到sigmoid函数中计算三元组的得分</p>
<h1>RMNN</h1>
<img src="https://img-blog.csdnimg.cn/9f4afcc1ddb84c77bd2160a6c5935f31.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202225935636" width="50%;">
<p>区别于DNN一开始时就将<strong>头实体向量与关系向量合并处理</strong>，RMNN选择的是在每一层让关系向量<strong>重新进入</strong>神经网络的隐含层中进行计算。从后面可以看出，这种结构在<strong>知识转移学习</strong>任务中具有优越性。</p>
<img src="https://img-blog.csdnimg.cn/513fc66fcbb9483a8071347e8232e327.png#pic_center" alt="image-20211202230744690" width="50%;">
<p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">W^{(l)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>B</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">B^{(l)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span> 分别表示第<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span>层正常权重矩阵和特定于<strong>关系</strong>（区别DNN）的权重矩阵</p>
<img src="https://img-blog.csdnimg.cn/e412b904eed34c028f96fc0d60690261.png#pic_center" alt="image-20211202231040838" width="50%;">
<p>最后，取最顶层的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>z</mi><mrow><mo stretchy="false">(</mo><mi>L</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">z^{(L)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.888em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.888em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">L</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span>代入得分函数进行计算</p>
<h1>实验</h1>
<h2 id="文本蕴含">文本蕴含</h2>
<img src="https://img-blog.csdnimg.cn/f3bd4f60e91e4c19bcb0f04c5a90366d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width="60%;">
<p>从结果可以看出，本文提出的基于DNN的NAM模型与各种传统方法相比有了很大的改进。这表明，通过在连续空间中表示句子并利用深度神经网络进行概率推理，可以更好地建模自然语言中的蕴涵关系。</p>
<h2 id="三元组分类">三元组分类</h2>
<img src="https://img-blog.csdnimg.cn/e8f284def046495bb9d514f96104b21c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202231715839" width="60%;">
<p>三元组分类：给定一个三元组判断其是正/反样本</p>
<p>首先设定一个临界值T，利用上述提出的得分函数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f()</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mclose">)</span></span></span></span>对一个三元组进行计算，若值大于临界值 T，则判断为正样本</p>
<p>结果清楚地表明，两种NAM方法(DNN和RMNN)在这些三级分类任务上取得了相当的性能，并且都比所有现有方法取得了一致的改进。特别是，与流行的神经张量网络(NTN)相比，RMNN模型在WN11和FB13上的绝对改进幅度分别为3.7%和1.9%</p>
<h2 id="常识推理">常识推理</h2>
<p>所谓常识推理，如骆驼是否能够穿越沙漠？提出的NAM模型通过计算关联概率Pr(E2|E1)来回答这个问题，其中E1 ={骆驼，能够}和E2 =穿越沙漠。</p>
<img src="https://img-blog.csdnimg.cn/df385e84eac7407c9efb2ee4facacd3d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202233038218" width="60%;">
<p>我们在图4中展示了CN14中所有14个关系对RNMM和NTN的分类精度，结果表明，RMNN的准确率在不同关系之间存在差异，从80.1% (desire)到93.5% (CreatedBy)。我们注意到一些常识关系(如欲望，能力，HasSubevent)比其他(如创造的，原因欲望，动机的目标)更难。总的来说，RNMM在几乎所有关系上都显著优于NTN。</p>
<h2 id="知识迁移学习">知识迁移学习</h2>
<img src="https://img-blog.csdnimg.cn/9c91093470eb4249b0dd7ec667fdf450.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" width="60%;">
<p>在实验中，作者使用CN14中的其他13个关系来训练NAM模型(DNN和RMNN)，来验证第14个关系的预测结果。在迁移学习过程中，作者<strong>冻结所有NAM参数</strong>，包括所有权重和实体表示，只从中学习一个新的CausesDesire关系代码。</p>
<p>结果表明，在这个实验中，RNMM比DNN表现得更好，我们可以显著改善RNMM的新关系，只需要5-20%的训练样本为CausesDesire。这表明连接关系代码到所有隐藏层的结构可以从相对较少的训练样本中更有效地学习新的关系代码。</p>
<img src="https://img-blog.csdnimg.cn/ba397b9cf8b6405589259055791496f9.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="image-20211202233557902" width="60%;">
<p>作者在迁移学习任务中，设置了第二个实验，<strong>更新所有NAM参数</strong>。</p>
<p>正如预期的那样，因为参数发生改变，原有的13个关系的表现将会恶化。</p>
<h1>总结</h1>
<p>在本文中，作者提出了用于概率推理的神经关联模型，包括了两种模型结构DNN和RMNN，来计算任意两个事件之间的关联概率。在多个推理任务上的实验结果表明，两个模型都能显著优于现有的推理方法。此外，RMNN在知识迁移方面表现突出。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">codeslogan</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://codeslogan.github.io">https://codeslogan.github.io</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">CodeSlogan</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML/">ML</a></div><div class="post_share"><div class="social-share" data-image="https://img-blog.csdnimg.cn/67f7a84d5cb04facb21f8cd45dc965ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center&quot; alt=&quot;image-20211202153543459" data-sites="facebook,twitter,`,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/images/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/images/wechat.jpg" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="/images/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/images/alipay.jpg" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/12/24/SpringBoot%E5%85%A5%E9%97%A8%E7%BA%A7%E9%A1%B9%E7%9B%AEhnucisys/"><img class="prev-cover" src="https://img-blog.csdnimg.cn/93e6b7eec55045d191768b4f30a4a65c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">SpringBoot入门级项目hnucisys</div></div></a></div><div class="next-post pull-right"><a href="/2021/11/25/Java3D%E8%AE%BE%E8%AE%A1%E4%BD%9C%E5%93%81%E5%85%A5%E9%97%A8%E7%BA%A7%E6%95%99%E7%A8%8B/"><img class="next-cover" src="https://img-blog.csdnimg.cn/d03e8c9f6cf84eb19c329853f8a0627e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_18,color_FFFFFF,t_70,g_se,x_16" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Java3D设计作品入门级教程</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/09/20/2021ML%E7%AC%94%E8%AE%B0/" title="ML笔记"><img class="cover" src="/2021/09/20/2021ML%E7%AC%94%E8%AE%B0/p0.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-09-20</div><div class="title">ML笔记</div></div></a></div><div><a href="/2021/10/10/Gaussian%20Embedding/" title="论文阅读Gaussian Embedding"><img class="cover" src="/2021/10/10/Gaussian%20Embedding/img1.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-10</div><div class="title">论文阅读Gaussian Embedding</div></div></a></div><div><a href="/2020/11/01/miniconda%E7%9A%84%E4%BD%BF%E7%94%A8/" title="miniconda的使用"><img class="cover" src="/2020/11/01/miniconda%E7%9A%84%E4%BD%BF%E7%94%A8/mini.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-11-01</div><div class="title">miniconda的使用</div></div></a></div><div><a href="/2021/11/25/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BBKnowledge%20Vault/" title="论文阅读Knowledge Vault"><img class="cover" src="https://img-blog.csdnimg.cn/b6c4822c5c65411eb10f9fadc04026e7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAQ29kZVNsb2dhbg==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-25</div><div class="title">论文阅读Knowledge Vault</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/1.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">codeslogan</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">33</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">23</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/CodeSlogan"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/CodeSlogan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:mrchen40@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">If not me, who?</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">1.</span> <span class="toc-text">论文题目归属及框架</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">2.</span> <span class="toc-text">背景与动机</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">3.</span> <span class="toc-text">贡献</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">4.</span> <span class="toc-text">统计关系学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">5.</span> <span class="toc-text">NAM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#loss"><span class="toc-number">5.1.</span> <span class="toc-text">loss</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">6.</span> <span class="toc-text">DNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">7.</span> <span class="toc-text">RMNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">8.</span> <span class="toc-text">实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E8%95%B4%E5%90%AB"><span class="toc-number">8.1.</span> <span class="toc-text">文本蕴含</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E5%85%83%E7%BB%84%E5%88%86%E7%B1%BB"><span class="toc-number">8.2.</span> <span class="toc-text">三元组分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%AF%86%E6%8E%A8%E7%90%86"><span class="toc-number">8.3.</span> <span class="toc-text">常识推理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.4.</span> <span class="toc-text">知识迁移学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link"><span class="toc-number">9.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/05/07/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E7%BA%A2%E9%BB%91%E6%A0%91%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" title="红黑树学习笔记">红黑树学习笔记</a><time datetime="2022-05-07T14:20:34.000Z" title="发表于 2022-05-07 22:20:34">2022-05-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/04/29/%E8%B5%9B%E5%90%8E%E5%9B%9E%E5%BF%86%E5%BD%95/" title="赛后回忆录">赛后回忆录</a><time datetime="2022-04-29T07:43:15.000Z" title="发表于 2022-04-29 15:43:15">2022-04-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/03/31/%E5%81%87%E5%B8%81%E9%97%AE%E9%A2%98(n%E6%9E%9A%E7%A1%AC%E5%B8%81DFS)/" title="假币问题(n枚硬币)">假币问题(n枚硬币)</a><time datetime="2022-03-31T13:40:24.000Z" title="发表于 2022-03-31 21:40:24">2022-03-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/03/12/%E8%93%9D%E6%A1%A5%E5%A4%87%E8%B5%9B/" title="冲出海岛-蓝桥">冲出海岛-蓝桥</a><time datetime="2022-03-12T08:35:27.000Z" title="发表于 2022-03-12 16:35:27">2022-03-12</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2022/02/28/%E6%95%B0%E8%AE%BA/" title="机试-数论">机试-数论</a><time datetime="2022-02-28T07:03:06.000Z" title="发表于 2022-02-28 15:03:06">2022-02-28</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By codeslogan</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my blog!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/copy-tex.css"><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>